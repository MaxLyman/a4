{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92dce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657d438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== In page:  0\n",
      "BioShock Infinite V: Skyrimntom Painssthe Revenants Edition -\r"
     ]
    }
   ],
   "source": [
    "pages = {}\n",
    "\n",
    "for page in range(200):\n",
    "    \n",
    "    data_page = {\n",
    "        'name':[],\n",
    "        'platform':[],\n",
    "        'r-date':[],\n",
    "        'score':[],\n",
    "        'user score':[],\n",
    "        'developer':[],\n",
    "        'genre':[],\n",
    "        'players':[],\n",
    "        'critics':[],\n",
    "        'users':[]\n",
    "    }    \n",
    "    \n",
    "    # Site inside metacritic listing \"Game Releases by Score\"\n",
    "    url = 'https://www.metacritic.com/browse/games/score/metascore/all/all/filtered?page='+str(page)\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Printing out current page\n",
    "    print(50*'=', \"In page: \", page)\n",
    "    \n",
    "    # Loop through all games in current page\n",
    "    for game in soup.find_all('td', class_ = 'clamp-summary-wrap'):\n",
    "        # Name\n",
    "        data_page['name'].append(game.find('h3').text)\n",
    "        \n",
    "        print(game.find('h3').text, end=\"\\r\")\n",
    "        \n",
    "        # Platform\n",
    "        platform = game.find('span', class_='data').text\n",
    "\n",
    "        # Removing white space\n",
    "        platform = platform.replace('\\n','')\n",
    "        platform = platform.replace(' ','')\n",
    "\n",
    "        data_page['platform'].append(platform)\n",
    "        \n",
    "        # Release date\n",
    "        data_page['r-date'].append(game.select('div.clamp-details span')[2].text)\n",
    "        \n",
    "        # MetaScore (has different classes depending on score)\n",
    "        score_list = [\n",
    "            game.find('div', class_='metascore_w large game positive'),\n",
    "            game.find('div', class_='metascore_w large game mixed'),\n",
    "            game.find('div', class_='metascore_w large game negative')\n",
    "        ]\n",
    "        \n",
    "        # Filtering not none element in the score_list\n",
    "        score = [s.text for s in score_list if s is not None][0]\n",
    "        \n",
    "        data_page['score'].append(score)\n",
    "        \n",
    "        # User Score (has different classes depending on score)\n",
    "        score_list = [\n",
    "            game.find('div', class_='metascore_w user large game positive'),\n",
    "            game.find('div', class_='metascore_w user large game mixed'),\n",
    "            game.find('div', class_='metascore_w user large game negative'),\n",
    "            game.find('div', class_='metascore_w user large game tbd')\n",
    "        ]\n",
    "        \n",
    "        # Filtering not none element in the score_list\n",
    "        score = [s.text for s in score_list if s is not None][0]\n",
    "        \n",
    "        data_page['user score'].append(score)\n",
    "        \n",
    "        # Into the game page\n",
    "        # Getting the url of the reviews page:\n",
    "        url_info = game.find('a', class_='title')['href']\n",
    "\n",
    "        url_info = 'https://www.metacritic.com'+url_info\n",
    "\n",
    "        # Getting into the game page:\n",
    "        response_info = requests.get(url_info, headers = user_agent)\n",
    "\n",
    "        soup_info = BeautifulSoup(response_info.text, 'html.parser')\n",
    "\n",
    "        # Get developer info\n",
    "\n",
    "        developer = soup_info.find('li', class_ = 'summary_detail developer')\n",
    "        \n",
    "        if developer is not None:\n",
    "            developer = developer.find('span',class_='data').text\n",
    "\n",
    "            developer = developer.replace('\\n','')\n",
    "            developer = developer.replace(' ','')  \n",
    "\n",
    "            data_page['developer'].append(developer)\n",
    "        else:\n",
    "            data_page['developer'].append('No info')\n",
    "\n",
    "        # Get genre info (multiple genres are separated by commas in our entry)\n",
    "\n",
    "        genres = soup_info.find('li', class_ = 'summary_detail product_genre')\n",
    "        \n",
    "        if genres is not None:\n",
    "            genres = genres.find_all('span', class_='data')\n",
    "            genre=''\n",
    "\n",
    "            for item in genres:\n",
    "                if genre:\n",
    "                    genre = genre + ',' + item.text\n",
    "                else:\n",
    "                    genre = item.text\n",
    "\n",
    "            data_page['genre'].append(genre)\n",
    "        else:\n",
    "            data_page['genre'].append('No info')\n",
    "\n",
    "        # Get number of players\n",
    "\n",
    "        players = soup_info.find('li', class_ = 'summary_detail product_players')\n",
    "        \n",
    "        if players is not None:\n",
    "            players = players.find('span',class_='data').text\n",
    "            data_page['players'].append(players)\n",
    "        else:\n",
    "            data_page['players'].append('No info')\n",
    "\n",
    "        # Get number of critics\n",
    "\n",
    "        critics = soup_info.find('div',class_='score_summary metascore_summary')\n",
    "        \n",
    "        if critics is not None:\n",
    "            critics = critics.find('div',class_='summary').find('a').find('span').text\n",
    "\n",
    "            if critics is not None:\n",
    "\n",
    "                critics = critics.replace('\\n','')\n",
    "                critics = critics.replace(' ','')  \n",
    "\n",
    "                data_page['critics'].append(critics)\n",
    "\n",
    "            else:\n",
    "                data_page['critics'].append('0')\n",
    "        else:\n",
    "            data_page['critics'].append('0')\n",
    "\n",
    "        # get number of users\n",
    "\n",
    "        users = soup_info.find('div',class_='details side_details')\n",
    "        \n",
    "        if users is not None:\n",
    "            users = users.find('div',class_='score_summary')\n",
    "\n",
    "            if users is not None:\n",
    "                users = users.find('span',class_='count').find('a')\n",
    "\n",
    "                if users is not None:\n",
    "                    users = users.text\n",
    "                    users = re.sub('\\ Ratings$', '', users)\n",
    "                    data_page['users'].append(users)\n",
    "                else:\n",
    "                    data_page['users'].append('0')\n",
    "            else:\n",
    "                data_page['users'].append('0')\n",
    "        else:\n",
    "            data_page['users'].append('0')\n",
    "            \n",
    "            \n",
    "    # create a dict entry to store the dataframe for each page\n",
    "    pages[str(page)] = pd.DataFrame(data_page)\n",
    "    \n",
    "    # export page data as csv\n",
    "    pages[str(page)].to_csv('games_data-page'+str(page)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all dataframes to concatenate\n",
    "frames = []\n",
    "\n",
    "for k,v in pages.items():\n",
    "    frames.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b67786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate.index = range(len(df_ultimate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75914e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate.to_csv('games-data_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0717850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate.sort_index()\n",
    "df_ultimate['r-date'] = pd.to_datetime(df_ultimate['r-date'])\n",
    "df_ultimate.sort_values('r-date',inplace=True)\n",
    "#df_ultimate.reset_index(inplace=True,drop=True)\n",
    "#df_ultimate.drop(['level_0','index'],1)\n",
    "df_ultimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f20c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate.loc[df_ultimate['name'].str.contains(\"throttle\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcf213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ultimate.to_csv('games-data_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c552f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
